{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SESSION_07.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Session 07 <table class=\"tfo-notebook-buttons\" align=\"right\" margin-top=-700px>\n",
        "<tr>\n",
        "  <td>\n",
        "      <img src=\"https://github.com/FawadAbbas12/Workshop_ses1/blob/main/logo.png?raw=true\"  width=\"60\" height=\"60\" /><p style='margin-left:25px'>&nbsp;&nbsp;NECOP</p>\n",
        "  </td>\n",
        "  <td></td>\n",
        "  <td>\n",
        "          <a target=\"_blank\" href=\"https://ist.edu.pk/\"> <img style=\"margin-top:20px\" width=\"60\" height=\"50\" src=\"https://github.com/FawadAbbas12/Workshop_ses1/blob/main/IST-LOGO.jpeg?raw=true\"><p style='margin-left:18px'>IST</p></a>\n",
        "  </td>\n",
        "  </tr>\n",
        " </table>"
      ],
      "metadata": {
        "id": "kEZF8AbzLM7I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generative Learning Models <table class=\"tfo-notebook-buttons\" align=\"right\" style=\"margin-top:-55px\">\n",
        "  <td>\n",
        "      <a target=\"_blank\" href=\"https://keras.io/search.html?query=recurrent%20neural%20network\"><CNTER><img src=\"https://external-content.duckduckgo.com/iu/?u=http%3A%2F%2Fadventuresinmachinelearning.com%2Fwp-content%2Fuploads%2F2017%2F05%2Fkeras-logo-small-wb-1.png&f=1&nofb=1\"  width=\"50\" height=\"50\" /><p style='margin-left:12px'>KERAS</p></CENTER></a>\n",
        "  </td>\n",
        " </table>"
      ],
      "metadata": {
        "id": "aHSGNWvaLVKg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Auto Encoders\n"
      ],
      "metadata": {
        "id": "HlQMqxasORzK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hi5KiUjIK5uz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pO6EiClWK5vA"
      },
      "outputs": [],
      "source": [
        "## Sampling layer for VAE mean,std\n",
        "class Sampling(layers.Layer):\n",
        "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a fashion mnist class.\"\"\"\n",
        "\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var = inputs\n",
        "        batch = tf.shape(z_mean)[0]\n",
        "        dim = tf.shape(z_mean)[1]\n",
        "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKDUPwpVK5vD"
      },
      "source": [
        "### Encoder & decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kf-SaaCVK5vJ"
      },
      "outputs": [],
      "source": [
        "def build_encoder(input_shape, latent_dim):\n",
        "    encoder_inputs = keras.Input(shape=input_shape)\n",
        "    x = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\n",
        "    x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(16, activation=\"relu\")(x)\n",
        "    z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
        "    z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
        "    z = Sampling()([z_mean, z_log_var])\n",
        "    encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
        "    return encoder\n",
        "\n",
        "def build_decoder(latent_dim):\n",
        "    latent_inputs = keras.Input(shape=(latent_dim,))\n",
        "    x = layers.Dense(7 * 7 * 64, activation=\"relu\")(latent_inputs)\n",
        "    x = layers.Reshape((7, 7, 64))(x)\n",
        "    x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "    x = layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "    decoder_outputs = layers.Conv2DTranspose(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
        "    decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
        "    return decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubf5dzO5K5vN"
      },
      "source": [
        "### Variation Auto Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-cskcrn3K5vS"
      },
      "outputs": [],
      "source": [
        "class VAE(keras.Model):\n",
        "    def __init__(self, encoder, decoder, **kwargs):\n",
        "        super(VAE, self).__init__(**kwargs)\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
        "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
        "            name=\"reconstruction_loss\"\n",
        "        )\n",
        "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [\n",
        "            self.total_loss_tracker,\n",
        "            self.reconstruction_loss_tracker,\n",
        "            self.kl_loss_tracker,\n",
        "        ]\n",
        "\n",
        "    def train_step(self, data):\n",
        "        with tf.GradientTape() as tape:\n",
        "            z_mean, z_log_var, z = self.encoder(data)\n",
        "            reconstruction = self.decoder(z)\n",
        "            reconstruction_loss = tf.reduce_mean(\n",
        "                tf.reduce_sum(\n",
        "                    keras.losses.binary_crossentropy(data, reconstruction), axis=(1, 2)\n",
        "                )\n",
        "            )\n",
        "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
        "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
        "            total_loss = reconstruction_loss + kl_loss\n",
        "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "        self.kl_loss_tracker.update_state(kl_loss)\n",
        "        return {\n",
        "            \"loss\": self.total_loss_tracker.result(),\n",
        "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6lr5c3SK5ve"
      },
      "source": [
        "### Train VAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZKFv54AK5vf"
      },
      "outputs": [],
      "source": [
        "(x_train, _), (x_test, _) = keras.datasets.fashion_mnist.load_data()\n",
        "mnist_fashion = np.concatenate([x_train, x_test], axis=0)\n",
        "mnist_fashion = np.expand_dims(mnist_fashion, -1).astype(\"float32\") / 255\n",
        "latent_dim = 2\n",
        "encoder = build_encoder((28, 28, 1), latent_dim)\n",
        "decoder = build_decoder(latent_dim)\n",
        "vae = VAE(encoder, decoder)\n",
        "vae.compile(optimizer=keras.optimizers.Adam())\n",
        "vae.fit(mnist_fashion, epochs=5, batch_size=128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "to45gLymK5vh"
      },
      "source": [
        "### display sampled fashion mnist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7jQ09ZkOK5vh"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_latent_space(vae, n=30, figsize=15):\n",
        "    # display a n*n 2D manifold of mnist_fashion\n",
        "    class_size = 28\n",
        "    scale = 1.0\n",
        "    figure = np.zeros((class_size * n, class_size * n))\n",
        "    # linearly spaced coordinates corresponding to the 2D plot\n",
        "    # of classes in the latent space\n",
        "    grid_x = np.linspace(-scale, scale, n)\n",
        "    grid_y = np.linspace(-scale, scale, n)[::-1]\n",
        "    for i, yi in enumerate(grid_y):\n",
        "        for j, xi in enumerate(grid_x):\n",
        "            z_sample = np.array([[xi, yi]])\n",
        "            x_decoded = vae.decoder.predict(z_sample)\n",
        "            f_class = x_decoded[0].reshape(class_size, class_size)\n",
        "            figure[\n",
        "                i * class_size : (i + 1) * class_size,\n",
        "                j * class_size : (j + 1) * class_size,\n",
        "            ] = f_class\n",
        "\n",
        "    plt.figure(figsize=(figsize, figsize))\n",
        "    start_range = class_size // 2\n",
        "    end_range = n * class_size + start_range\n",
        "    pixel_range = np.arange(start_range, end_range, class_size)\n",
        "    sample_range_x = np.round(grid_x, 1)\n",
        "    sample_range_y = np.round(grid_y, 1)\n",
        "    plt.xticks(pixel_range, sample_range_x)\n",
        "    plt.yticks(pixel_range, sample_range_y)\n",
        "    plt.xlabel(\"z[0]\")\n",
        "    plt.ylabel(\"z[1]\")\n",
        "    plt.imshow(figure, cmap=\"Greys_r\")\n",
        "    plt.show()\n",
        "\n",
        "plot_latent_space(vae, 10, figsize=10)\n",
        "\n",
        "#######################################\n",
        "# exercise 1: plot for only 5 samples #\n",
        "#######################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nQlAnaPK5vi"
      },
      "source": [
        "### Different fashion mnist classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WhVJThQ-K5vi"
      },
      "outputs": [],
      "source": [
        "def plot_label_clusters(vae, data, labels):\n",
        "    # display a 2D plot of the mnist fashion classes in the latent space\n",
        "    z_mean, _, _ = vae.encoder.predict(data)\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    plt.scatter(z_mean[:, 0], z_mean[:, 1], c=labels)\n",
        "    plt.colorbar()\n",
        "    plt.xlabel(\"z[0]\")\n",
        "    plt.ylabel(\"z[1]\")\n",
        "    plt.show()\n",
        "\n",
        "(x_train, y_train), _ = keras.datasets.mnist.load_data()\n",
        "x_train = np.expand_dims(x_train, -1).astype(\"float32\") / 255\n",
        "\n",
        "plot_label_clusters(vae, x_train[1:200], y_train[1:200])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### train for more epochs\n"
      ],
      "metadata": {
        "id": "SQrTBX-YZSEi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ################################################\n",
        "# # exercise 1 After session uncomment following #\n",
        "# # code blocks and retrain model                #\n",
        "# ############################################## #\n",
        "# class VAE_MONITOR(tf.keras.callbacks.Callback):\n",
        "#     def __init__(self, nSample, figsize):\n",
        "#         self.nSample = nSample\n",
        "#         self.figsize = figsize\n",
        "\n",
        "#     def on_epoch_end(self, epoch, logs=None):\n",
        "#         self.plot_latent_space(epoch)\n",
        "\n",
        "#     def plot_latent_space(self, epoch):\n",
        "#         # display a n*n 2D manifold of mnist_fashion\n",
        "#         class_size = 28\n",
        "#         scale = 1.0\n",
        "#         figure = np.zeros((class_size * self.nSample, class_size * self.nSample))\n",
        "#         grid_x = np.linspace(-scale, scale,  self.nSample)\n",
        "#         grid_y = np.linspace(-scale, scale,  self.nSample)[::-1]\n",
        "#         for i, yi in enumerate(grid_y):\n",
        "#             for j, xi in enumerate(grid_x):\n",
        "#                 z_sample = np.array([[xi, yi]])\n",
        "#                 x_decoded = self.model.decoder.predict(z_sample)\n",
        "#                 f_class = x_decoded[0].reshape(class_size, class_size)\n",
        "#                 figure[\n",
        "#                     i * class_size : (i + 1) * class_size,\n",
        "#                     j * class_size : (j + 1) * class_size,\n",
        "#                 ] = f_class\n",
        "\n",
        "#         plt.figure(figsize=(self.figsize, self.figsize))\n",
        "#         start_range = class_size // 2\n",
        "#         pixel_range = np.arange(start_range, self.nSample * class_size + start_range, class_size)\n",
        "#         sample_range_x = np.round(grid_x, 1)\n",
        "#         sample_range_y = np.round(grid_y, 1)\n",
        "#         plt.xticks(pixel_range, sample_range_x)\n",
        "#         plt.yticks(pixel_range, sample_range_y)\n",
        "#         plt.xlabel(\"z[0]\")\n",
        "#         plt.ylabel(\"z[1]\")\n",
        "#         plt.imshow(figure, cmap=\"Greys_r\")\n",
        "#         plt.title(f\"Epoch: {epoch}\")\n",
        "#         plt.show()\n",
        "# latent_dim = 2\n",
        "# encoder = build_encoder((28, 28, 1), latent_dim)\n",
        "# decoder = build_decoder(latent_dim)\n",
        "# vae = VAE(encoder, decoder)\n",
        "# vae.compile(optimizer=keras.optimizers.Adam())\n",
        "# vae.fit(mnist_fashion, epochs=1, callbacks=[VAE_MONITOR(nSample=10, figsize=5)], batch_size=128)"
      ],
      "metadata": {
        "id": "dpLc2WgFbYUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_zBNNlHipPgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generative Adversarial Neural networks "
      ],
      "metadata": {
        "id": "4b3dQhSFzUwj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_j9vJIjkncj4"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import gdown\n",
        "from zipfile import ZipFile\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKCnJ--xncl2"
      },
      "source": [
        "### Generator & Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANGGJF2incmJ"
      },
      "outputs": [],
      "source": [
        "def build_discriminator(input_shape):\n",
        "    discriminator = keras.Sequential([\n",
        "            keras.Input(shape=input_shape),\n",
        "            layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\"),\n",
        "            layers.LeakyReLU(alpha=0.2),\n",
        "            layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "            layers.LeakyReLU(alpha=0.2),\n",
        "            layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "            layers.LeakyReLU(alpha=0.2),\n",
        "            layers.Flatten(),\n",
        "            layers.Dropout(0.2),\n",
        "            layers.Dense(1, activation=\"sigmoid\"),\n",
        "        ],\n",
        "        name=\"discriminator\",)\n",
        "    return discriminator\n",
        "def build_generator(latent_dim):\n",
        "    generator = keras.Sequential([\n",
        "        keras.Input(shape=(latent_dim,)),\n",
        "        layers.Dense(8 * 8 * 128),\n",
        "        layers.Reshape((8, 8, 128)),\n",
        "        layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2DTranspose(512, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2D(3, kernel_size=5, padding=\"same\", activation=\"sigmoid\"),\n",
        "        ],\n",
        "        name=\"generator\",)\n",
        "    return generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rv53rfLTncmh"
      },
      "source": [
        "### GAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQZ8Ktm5ncm5"
      },
      "outputs": [],
      "source": [
        "class GAN(keras.Model):\n",
        "    def __init__(self, discriminator, generator, latent_dim):\n",
        "        super(GAN, self).__init__()\n",
        "        self.discriminator = discriminator\n",
        "        self.generator = generator\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
        "        super(GAN, self).compile()\n",
        "        self.d_optimizer = d_optimizer\n",
        "        self.g_optimizer = g_optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "        self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")\n",
        "        self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.d_loss_metric, self.g_loss_metric]\n",
        "\n",
        "    def train_step(self, real_images):\n",
        "        # Sample random points in the latent space\n",
        "        batch_size = tf.shape(real_images)[0]\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        # Decode them to fake images\n",
        "        generated_images = self.generator(random_latent_vectors)\n",
        "\n",
        "        # Combine them with real images\n",
        "        combined_images = tf.concat([generated_images, real_images], axis=0)\n",
        "\n",
        "        # Assemble labels discriminating real from fake images\n",
        "        labels = tf.concat(\n",
        "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
        "        )\n",
        "        # Add random noise to the labels - important trick!\n",
        "        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
        "\n",
        "        # Train the discriminator\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(combined_images)\n",
        "            d_loss = self.loss_fn(labels, predictions)\n",
        "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
        "        self.d_optimizer.apply_gradients(\n",
        "            zip(grads, self.discriminator.trainable_weights)\n",
        "        )\n",
        "\n",
        "        # Sample random points in the latent space\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        # Assemble labels that say \"all real images\"\n",
        "        misleading_labels = tf.zeros((batch_size, 1))\n",
        "\n",
        "        # Train the generator (note that we should *not* update the weights\n",
        "        # of the discriminator)!\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(self.generator(random_latent_vectors))\n",
        "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
        "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
        "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
        "\n",
        "        # Update metrics\n",
        "        self.d_loss_metric.update_state(d_loss)\n",
        "        self.g_loss_metric.update_state(g_loss)\n",
        "        return {\n",
        "            \"d_loss\": self.d_loss_metric.result(),\n",
        "            \"g_loss\": self.g_loss_metric.result(),\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eURuDedHncnE"
      },
      "outputs": [],
      "source": [
        "#callback Class for monitoring gan's performence after every epoch\n",
        "class GANMonitor(keras.callbacks.Callback):\n",
        "    def __init__(self, num_img=3, latent_dim=128):\n",
        "        self.num_img = num_img\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
        "        generated_images = self.model.generator(random_latent_vectors)\n",
        "        generated_images *= 255\n",
        "        generated_images.numpy()\n",
        "        for i in range(self.num_img):\n",
        "            img = keras.preprocessing.image.array_to_img(generated_images[i])\n",
        "            img.save(\"generated_img_%03d_%d.png\" % (epoch, i))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download dataset"
      ],
      "metadata": {
        "id": "rhfFkRcV9Zls"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_pHFjtOncnH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gdown\n",
        "from zipfile import ZipFile\n",
        "os.makedirs(\"celeba_gan\")\n",
        "\n",
        "url = \"https://drive.google.com/uc?id=1O7m1010EJjLE5QxLZiM9Fpjs7Oj6e684\"\n",
        "output = \"celeba_gan/data.zip\"\n",
        "gdown.download(url, output, quiet=True)\n",
        "\n",
        "with ZipFile(\"celeba_gan/data.zip\", \"r\") as zipobj:\n",
        "    zipobj.extractall(\"celeba_gan\")\n",
        "\n",
        "dataset = keras.preprocessing.image_dataset_from_directory(\n",
        "    \"./celeba_gan/img_align_celeba/\", label_mode=None, image_size=(64, 64), batch_size=32\n",
        ")\n",
        "dataset = dataset.map(lambda x: x / 255.0)    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNnMd1zvncnW"
      },
      "outputs": [],
      "source": [
        "for x in dataset:\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow((x.numpy() * 255).astype(\"int32\")[0])\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3__6MY57ncnb"
      },
      "source": [
        "### Train Gan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Wudyptfncni"
      },
      "outputs": [],
      "source": [
        "epochs = 1  # In practice, use ~100 epochs\n",
        "latent_dim = 128\n",
        "discriminator = build_discriminator((64, 64, 3))\n",
        "generator = build_generator(latent_dim)\n",
        "\n",
        "gan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\n",
        "\n",
        "gan.compile(\n",
        "    d_optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
        "    g_optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
        "    loss_fn=keras.losses.BinaryCrossentropy(),\n",
        ")\n",
        "\n",
        "gan.fit(\n",
        "    dataset, epochs=epochs, callbacks=[GANMonitor(num_img=10, latent_dim=latent_dim)]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## stylegan"
      ],
      "metadata": {
        "id": "2KR-EQ6OnocX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tips and Advance concepts 👇\n",
        "\n"
      ],
      "metadata": {
        "id": "9z4IAPmbFwF0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conditional GAN"
      ],
      "metadata": {
        "id": "wbqAwoid43QQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "!pip install -q git+https://github.com/tensorflow/docs >/dev/null\n",
        "from tensorflow_docs.vis import embed\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import imageio"
      ],
      "metadata": {
        "id": "wvv2PLlK89yd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We'll use all the available examples from both the training and test\n",
        "# sets.\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "all_digits = np.concatenate([x_train, x_test])\n",
        "all_labels = np.concatenate([y_train, y_test])\n",
        "\n",
        "# Scale the pixel values to [0, 1] range, add a channel dimension to\n",
        "# the images, and one-hot encode the labels.\n",
        "all_digits = all_digits.astype(\"float32\") / 255.0\n",
        "all_digits = np.reshape(all_digits, (-1, 28, 28, 1))\n",
        "all_labels = keras.utils.to_categorical(all_labels, 10)\n",
        "\n",
        "# Create tf.data.Dataset.\n",
        "dataset = tf.data.Dataset.from_tensor_slices((all_digits, all_labels))\n",
        "dataset = dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
        "\n",
        "print(f\"Shape of training images: {all_digits.shape}\")\n",
        "print(f\"Shape of training labels: {all_labels.shape}\")"
      ],
      "metadata": {
        "id": "Emos5eoa9CXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the discriminator.\n",
        "def get_discriminator(discriminator_in_channels):\n",
        "    return keras.Sequential([\n",
        "            keras.layers.InputLayer((28, 28, discriminator_in_channels)),\n",
        "            layers.Conv2D(64, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "            layers.LeakyReLU(alpha=0.2),\n",
        "            layers.Conv2D(128, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "            layers.LeakyReLU(alpha=0.2),\n",
        "            layers.GlobalMaxPooling2D(),\n",
        "            layers.Dense(1),],\n",
        "            name=\"discriminator\",\n",
        "        )\n",
        "\n",
        "# Create the generator.\n",
        "def get_generator(generator_in_channels):\n",
        "    return keras.Sequential([\n",
        "            keras.layers.InputLayer((generator_in_channels,)),\n",
        "            # We want to generate 128 + num_classes coefficients to reshape into a\n",
        "            # 7x7x(128 + num_classes) map.\n",
        "            layers.Dense(7 * 7 * generator_in_channels),\n",
        "            layers.LeakyReLU(alpha=0.2),\n",
        "            layers.Reshape((7, 7, generator_in_channels)),\n",
        "            layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n",
        "            layers.LeakyReLU(alpha=0.2),\n",
        "            layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n",
        "            layers.LeakyReLU(alpha=0.2),\n",
        "            layers.Conv2D(1, (7, 7), padding=\"same\", activation=\"sigmoid\"),\n",
        "        ],\n",
        "        name=\"generator\",)\n"
      ],
      "metadata": {
        "id": "8Id6XMrf9Ola"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConditionalGAN(keras.Model):\n",
        "    def __init__(self, discriminator, generator, latent_dim):\n",
        "        super(ConditionalGAN, self).__init__()\n",
        "        self.discriminator = discriminator\n",
        "        self.generator = generator\n",
        "        self.latent_dim = latent_dim\n",
        "        self.gen_loss_tracker = keras.metrics.Mean(name=\"generator_loss\")\n",
        "        self.disc_loss_tracker = keras.metrics.Mean(name=\"discriminator_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.gen_loss_tracker, self.disc_loss_tracker]\n",
        "\n",
        "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
        "        super(ConditionalGAN, self).compile()\n",
        "        self.d_optimizer = d_optimizer\n",
        "        self.g_optimizer = g_optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "\n",
        "    def train_step(self, data):\n",
        "        # Unpack the data.\n",
        "        real_images, one_hot_labels = data\n",
        "\n",
        "        # Add dummy dimensions to the labels so that they can be concatenated with\n",
        "        # the images. This is for the discriminator.\n",
        "        image_one_hot_labels = one_hot_labels[:, :, None, None]\n",
        "        image_one_hot_labels = tf.repeat(\n",
        "            image_one_hot_labels, repeats=[image_size * image_size]\n",
        "        )\n",
        "        image_one_hot_labels = tf.reshape(\n",
        "            image_one_hot_labels, (-1, image_size, image_size, num_classes)\n",
        "        )\n",
        "\n",
        "        # Sample random points in the latent space and concatenate the labels.\n",
        "        # This is for the generator.\n",
        "        batch_size = tf.shape(real_images)[0]\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "        random_vector_labels = tf.concat(\n",
        "            [random_latent_vectors, one_hot_labels], axis=1\n",
        "        )\n",
        "\n",
        "        # Decode the noise (guided by labels) to fake images.\n",
        "        generated_images = self.generator(random_vector_labels)\n",
        "\n",
        "        # Combine them with real images. Note that we are concatenating the labels\n",
        "        # with these images here.\n",
        "        fake_image_and_labels = tf.concat([generated_images, image_one_hot_labels], -1)\n",
        "        real_image_and_labels = tf.concat([real_images, image_one_hot_labels], -1)\n",
        "        combined_images = tf.concat(\n",
        "            [fake_image_and_labels, real_image_and_labels], axis=0\n",
        "        )\n",
        "\n",
        "        # Assemble labels discriminating real from fake images.\n",
        "        labels = tf.concat(\n",
        "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
        "        )\n",
        "\n",
        "        # Train the discriminator.\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(combined_images)\n",
        "            d_loss = self.loss_fn(labels, predictions)\n",
        "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
        "        self.d_optimizer.apply_gradients(\n",
        "            zip(grads, self.discriminator.trainable_weights)\n",
        "        )\n",
        "\n",
        "        # Sample random points in the latent space.\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "        random_vector_labels = tf.concat(\n",
        "            [random_latent_vectors, one_hot_labels], axis=1\n",
        "        )\n",
        "\n",
        "        # Assemble labels that say \"all real images\".\n",
        "        misleading_labels = tf.zeros((batch_size, 1))\n",
        "\n",
        "        # Train the generator (note that we should *not* update the weights\n",
        "        # of the discriminator)!\n",
        "        with tf.GradientTape() as tape:\n",
        "            fake_images = self.generator(random_vector_labels)\n",
        "            fake_image_and_labels = tf.concat([fake_images, image_one_hot_labels], -1)\n",
        "            predictions = self.discriminator(fake_image_and_labels)\n",
        "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
        "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
        "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
        "\n",
        "        # Monitor loss.\n",
        "        self.gen_loss_tracker.update_state(g_loss)\n",
        "        self.disc_loss_tracker.update_state(d_loss)\n",
        "        return {\n",
        "            \"g_loss\": self.gen_loss_tracker.result(),\n",
        "            \"d_loss\": self.disc_loss_tracker.result(),\n",
        "        }\n"
      ],
      "metadata": {
        "id": "Wjpo8KieIS8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "num_channels = 1\n",
        "num_classes = 10\n",
        "image_size = 28\n",
        "latent_dim = 128\n",
        "generator_in_channels = latent_dim + num_classes\n",
        "discriminator_in_channels = num_channels + num_classes\n",
        "\n",
        "cond_gan = ConditionalGAN(\n",
        "    discriminator=get_discriminator(discriminator_in_channels), generator=get_generator(generator_in_channels), latent_dim=latent_dim\n",
        ")\n",
        "cond_gan.compile(\n",
        "    d_optimizer=keras.optimizers.Adam(learning_rate=0.0003),\n",
        "    g_optimizer=keras.optimizers.Adam(learning_rate=0.0003),\n",
        "    loss_fn=keras.losses.BinaryCrossentropy(from_logits=True),\n",
        ")\n",
        "\n",
        "cond_gan.fit(dataset, epochs=20)\n"
      ],
      "metadata": {
        "id": "vaGTFiFDGRNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "C-GAN"
      ],
      "metadata": {
        "id": "wJhyH4w--bNs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConditionalGAN(keras.Model):\n",
        "    def __init__(self, discriminator, generator, latent_dim):\n",
        "        super(ConditionalGAN, self).__init__()\n",
        "        self.discriminator = discriminator\n",
        "        self.generator = generator\n",
        "        self.latent_dim = latent_dim\n",
        "        self.gen_loss_tracker = keras.metrics.Mean(name=\"generator_loss\")\n",
        "        self.disc_loss_tracker = keras.metrics.Mean(name=\"discriminator_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.gen_loss_tracker, self.disc_loss_tracker]\n",
        "\n",
        "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
        "        super(ConditionalGAN, self).compile()\n",
        "        self.d_optimizer = d_optimizer\n",
        "        self.g_optimizer = g_optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "\n",
        "    def train_step(self, data):\n",
        "        # Unpack the data.\n",
        "        real_images, one_hot_labels = data\n",
        "\n",
        "        # Add dummy dimensions to the labels so that they can be concatenated with\n",
        "        # the images. This is for the discriminator.\n",
        "        image_one_hot_labels = one_hot_labels[:, :, None, None]\n",
        "        image_one_hot_labels = tf.repeat(\n",
        "            image_one_hot_labels, repeats=[image_size * image_size]\n",
        "        )\n",
        "        image_one_hot_labels = tf.reshape(\n",
        "            image_one_hot_labels, (-1, image_size, image_size, num_classes)\n",
        "        )\n",
        "\n",
        "        # Sample random points in the latent space and concatenate the labels.\n",
        "        # This is for the generator.\n",
        "        batch_size = tf.shape(real_images)[0]\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "        random_vector_labels = tf.concat(\n",
        "            [random_latent_vectors, one_hot_labels], axis=1\n",
        "        )\n",
        "\n",
        "        # Decode the noise (guided by labels) to fake images.\n",
        "        generated_images = self.generator(random_vector_labels)\n",
        "\n",
        "        # Combine them with real images. Note that we are concatenating the labels\n",
        "        # with these images here.\n",
        "        fake_image_and_labels = tf.concat([generated_images, image_one_hot_labels], -1)\n",
        "        real_image_and_labels = tf.concat([real_images, image_one_hot_labels], -1)\n",
        "        combined_images = tf.concat(\n",
        "            [fake_image_and_labels, real_image_and_labels], axis=0\n",
        "        )\n",
        "\n",
        "        # Assemble labels discriminating real from fake images.\n",
        "        labels = tf.concat(\n",
        "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
        "        )\n",
        "\n",
        "        # Train the discriminator.\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(combined_images)\n",
        "            d_loss = self.loss_fn(labels, predictions)\n",
        "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
        "        self.d_optimizer.apply_gradients(\n",
        "            zip(grads, self.discriminator.trainable_weights)\n",
        "        )\n",
        "\n",
        "        # Sample random points in the latent space.\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "        random_vector_labels = tf.concat(\n",
        "            [random_latent_vectors, one_hot_labels], axis=1\n",
        "        )\n",
        "\n",
        "        # Assemble labels that say \"all real images\".\n",
        "        misleading_labels = tf.zeros((batch_size, 1))\n",
        "\n",
        "        # Train the generator (note that we should *not* update the weights\n",
        "        # of the discriminator)!\n",
        "        with tf.GradientTape() as tape:\n",
        "            fake_images = self.generator(random_vector_labels)\n",
        "            fake_image_and_labels = tf.concat([fake_images, image_one_hot_labels], -1)\n",
        "            predictions = self.discriminator(fake_image_and_labels)\n",
        "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
        "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
        "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
        "\n",
        "        # Monitor loss.\n",
        "        self.gen_loss_tracker.update_state(g_loss)\n",
        "        self.disc_loss_tracker.update_state(d_loss)\n",
        "        return {\n",
        "            \"g_loss\": self.gen_loss_tracker.result(),\n",
        "            \"d_loss\": self.disc_loss_tracker.result(),\n",
        "        }\n"
      ],
      "metadata": {
        "id": "_6iMd3i399U_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator_in_channels = latent_dim + num_classes\n",
        "discriminator_in_channels = num_channels + num_classes\n",
        "print(generator_in_channels, discriminator_in_channels)"
      ],
      "metadata": {
        "id": "iKVqrz2299HU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cycle GAN"
      ],
      "metadata": {
        "id": "feSMPRsF85zT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "!pip install tensorflow_addons >/dev/null\n",
        "import tensorflow_addons as tfa\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "tfds.disable_progress_bar()\n",
        "autotune = tf.data.AUTOTUNE\n",
        "\n",
        "\n",
        "# Load the horse-zebra dataset using tensorflow-datasets.\n",
        "dataset, _ = tfds.load(\"cycle_gan/horse2zebra\", with_info=True, as_supervised=True)\n",
        "train_horses, train_zebras = dataset[\"trainA\"], dataset[\"trainB\"]\n",
        "test_horses, test_zebras = dataset[\"testA\"], dataset[\"testB\"]\n",
        "\n",
        "# Define the standard image size.\n",
        "orig_img_size = (286, 286)\n",
        "# Size of the random crops to be used during training.\n",
        "input_img_size = (256, 256, 3)\n",
        "# Weights initializer for the layers.\n",
        "kernel_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
        "# Gamma initializer for instance normalization.\n",
        "gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
        "\n",
        "buffer_size = 256\n",
        "batch_size = 1\n",
        "\n",
        "\n",
        "def normalize_img(img):\n",
        "    img = tf.cast(img, dtype=tf.float32)\n",
        "    # Map values in the range [-1, 1]\n",
        "    return (img / 127.5) - 1.0\n",
        "\n",
        "\n",
        "def preprocess_train_image(img, label):\n",
        "    # Random flip\n",
        "    img = tf.image.random_flip_left_right(img)\n",
        "    # Resize to the original size first\n",
        "    img = tf.image.resize(img, [*orig_img_size])\n",
        "    # Random crop to 256X256\n",
        "    img = tf.image.random_crop(img, size=[*input_img_size])\n",
        "    # Normalize the pixel values in the range [-1, 1]\n",
        "    img = normalize_img(img)\n",
        "    return img\n",
        "\n",
        "\n",
        "def preprocess_test_image(img, label):\n",
        "    # Only resizing and normalization for the test images.\n",
        "    img = tf.image.resize(img, [input_img_size[0], input_img_size[1]])\n",
        "    img = normalize_img(img)\n",
        "    return img\n",
        "\n",
        "train_horses = (\n",
        "    train_horses.map(preprocess_train_image, num_parallel_calls=autotune)\n",
        "    .cache()\n",
        "    .shuffle(buffer_size)\n",
        "    .batch(batch_size)\n",
        ")\n",
        "train_zebras = (\n",
        "    train_zebras.map(preprocess_train_image, num_parallel_calls=autotune)\n",
        "    .cache()\n",
        "    .shuffle(buffer_size)\n",
        "    .batch(batch_size)\n",
        ")\n",
        "\n",
        "# Apply the preprocessing operations to the test data\n",
        "test_horses = (\n",
        "    test_horses.map(preprocess_test_image, num_parallel_calls=autotune)\n",
        "    .cache()\n",
        "    .shuffle(buffer_size)\n",
        "    .batch(batch_size)\n",
        ")\n",
        "test_zebras = (\n",
        "    test_zebras.map(preprocess_test_image, num_parallel_calls=autotune)\n",
        "    .cache()\n",
        "    .shuffle(buffer_size)\n",
        "    .batch(batch_size)\n",
        ")\n"
      ],
      "metadata": {
        "id": "j9Tf9-co42yv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helper layers"
      ],
      "metadata": {
        "id": "Q_AvAoP57j2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReflectionPadding2D(layers.Layer):\n",
        "    \"\"\"Implements Reflection Padding as a layer.\n",
        "\n",
        "    Args:\n",
        "        padding(tuple): Amount of padding for the\n",
        "        spatial dimensions.\n",
        "\n",
        "    Returns:\n",
        "        A padded tensor with the same type as the input tensor.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, padding=(1, 1), **kwargs):\n",
        "        self.padding = tuple(padding)\n",
        "        super(ReflectionPadding2D, self).__init__(**kwargs)\n",
        "\n",
        "    def call(self, input_tensor, mask=None):\n",
        "        padding_width, padding_height = self.padding\n",
        "        padding_tensor = [\n",
        "            [0, 0],\n",
        "            [padding_height, padding_height],\n",
        "            [padding_width, padding_width],\n",
        "            [0, 0],\n",
        "        ]\n",
        "        return tf.pad(input_tensor, padding_tensor, mode=\"REFLECT\")\n",
        "\n",
        "\n",
        "def residual_block(\n",
        "    x,\n",
        "    activation,\n",
        "    kernel_initializer=kernel_init,\n",
        "    kernel_size=(3, 3),\n",
        "    strides=(1, 1),\n",
        "    padding=\"valid\",\n",
        "    gamma_initializer=gamma_init,\n",
        "    use_bias=False,\n",
        "):\n",
        "    dim = x.shape[-1]\n",
        "    input_tensor = x\n",
        "\n",
        "    x = ReflectionPadding2D()(input_tensor)\n",
        "    x = layers.Conv2D(\n",
        "        dim,\n",
        "        kernel_size,\n",
        "        strides=strides,\n",
        "        kernel_initializer=kernel_initializer,\n",
        "        padding=padding,\n",
        "        use_bias=use_bias,\n",
        "    )(x)\n",
        "    x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(x)\n",
        "    x = activation(x)\n",
        "\n",
        "    x = ReflectionPadding2D()(x)\n",
        "    x = layers.Conv2D(\n",
        "        dim,\n",
        "        kernel_size,\n",
        "        strides=strides,\n",
        "        kernel_initializer=kernel_initializer,\n",
        "        padding=padding,\n",
        "        use_bias=use_bias,\n",
        "    )(x)\n",
        "    x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(x)\n",
        "    x = layers.add([input_tensor, x])\n",
        "    return x\n",
        "\n",
        "\n",
        "def downsample(\n",
        "    x,\n",
        "    filters,\n",
        "    activation,\n",
        "    kernel_initializer=kernel_init,\n",
        "    kernel_size=(3, 3),\n",
        "    strides=(2, 2),\n",
        "    padding=\"same\",\n",
        "    gamma_initializer=gamma_init,\n",
        "    use_bias=False,\n",
        "):\n",
        "    x = layers.Conv2D(\n",
        "        filters,\n",
        "        kernel_size,\n",
        "        strides=strides,\n",
        "        kernel_initializer=kernel_initializer,\n",
        "        padding=padding,\n",
        "        use_bias=use_bias,\n",
        "    )(x)\n",
        "    x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(x)\n",
        "    if activation:\n",
        "        x = activation(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def upsample(\n",
        "    x,\n",
        "    filters,\n",
        "    activation,\n",
        "    kernel_size=(3, 3),\n",
        "    strides=(2, 2),\n",
        "    padding=\"same\",\n",
        "    kernel_initializer=kernel_init,\n",
        "    gamma_initializer=gamma_init,\n",
        "    use_bias=False,\n",
        "):\n",
        "    x = layers.Conv2DTranspose(\n",
        "        filters,\n",
        "        kernel_size,\n",
        "        strides=strides,\n",
        "        padding=padding,\n",
        "        kernel_initializer=kernel_initializer,\n",
        "        use_bias=use_bias,\n",
        "    )(x)\n",
        "    x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(x)\n",
        "    if activation:\n",
        "        x = activation(x)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "jLDXon4i5q6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generator and discriminator"
      ],
      "metadata": {
        "id": "BTcLPlaR56NG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_resnet_generator(\n",
        "    filters=64,\n",
        "    num_downsampling_blocks=2,\n",
        "    num_residual_blocks=9,\n",
        "    num_upsample_blocks=2,\n",
        "    gamma_initializer=gamma_init,\n",
        "    name=None,\n",
        "):\n",
        "    img_input = layers.Input(shape=input_img_size, name=name + \"_img_input\")\n",
        "    x = ReflectionPadding2D(padding=(3, 3))(img_input)\n",
        "    x = layers.Conv2D(filters, (7, 7), kernel_initializer=kernel_init, use_bias=False)(\n",
        "        x\n",
        "    )\n",
        "    x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(x)\n",
        "    x = layers.Activation(\"relu\")(x)\n",
        "\n",
        "    # Downsampling\n",
        "    for _ in range(num_downsampling_blocks):\n",
        "        filters *= 2\n",
        "        x = downsample(x, filters=filters, activation=layers.Activation(\"relu\"))\n",
        "\n",
        "    # Residual blocks\n",
        "    for _ in range(num_residual_blocks):\n",
        "        x = residual_block(x, activation=layers.Activation(\"relu\"))\n",
        "\n",
        "    # Upsampling\n",
        "    for _ in range(num_upsample_blocks):\n",
        "        filters //= 2\n",
        "        x = upsample(x, filters, activation=layers.Activation(\"relu\"))\n",
        "\n",
        "    # Final block\n",
        "    x = ReflectionPadding2D(padding=(3, 3))(x)\n",
        "    x = layers.Conv2D(3, (7, 7), padding=\"valid\")(x)\n",
        "    x = layers.Activation(\"tanh\")(x)\n",
        "\n",
        "    model = keras.models.Model(img_input, x, name=name)\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_discriminator(\n",
        "    filters=64, kernel_initializer=kernel_init, num_downsampling=3, name=None\n",
        "):\n",
        "    img_input = layers.Input(shape=input_img_size, name=name + \"_img_input\")\n",
        "    x = layers.Conv2D(\n",
        "        filters,\n",
        "        (4, 4),\n",
        "        strides=(2, 2),\n",
        "        padding=\"same\",\n",
        "        kernel_initializer=kernel_initializer,\n",
        "    )(img_input)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "\n",
        "    num_filters = filters\n",
        "    for num_downsample_block in range(3):\n",
        "        num_filters *= 2\n",
        "        if num_downsample_block < 2:\n",
        "            x = downsample(\n",
        "                x,\n",
        "                filters=num_filters,\n",
        "                activation=layers.LeakyReLU(0.2),\n",
        "                kernel_size=(4, 4),\n",
        "                strides=(2, 2),\n",
        "            )\n",
        "        else:\n",
        "            x = downsample(\n",
        "                x,\n",
        "                filters=num_filters,\n",
        "                activation=layers.LeakyReLU(0.2),\n",
        "                kernel_size=(4, 4),\n",
        "                strides=(1, 1),\n",
        "            )\n",
        "\n",
        "    x = layers.Conv2D(\n",
        "        1, (4, 4), strides=(1, 1), padding=\"same\", kernel_initializer=kernel_initializer\n",
        "    )(x)\n",
        "\n",
        "    model = keras.models.Model(inputs=img_input, outputs=x, name=name)\n",
        "    return model\n",
        "\n",
        "\n",
        "# Get the generators\n",
        "gen_G = get_resnet_generator(name=\"generator_G\")\n",
        "gen_F = get_resnet_generator(name=\"generator_F\")\n",
        "\n",
        "# Get the discriminators\n",
        "disc_X = get_discriminator(name=\"discriminator_X\")\n",
        "disc_Y = get_discriminator(name=\"discriminator_Y\")\n"
      ],
      "metadata": {
        "id": "qvJ-AM0v5_er"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cycle GAN "
      ],
      "metadata": {
        "id": "FEWRi9bk8lOT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CycleGan(keras.Model):\n",
        "    def __init__(\n",
        "        self,\n",
        "        generator_G,\n",
        "        generator_F,\n",
        "        discriminator_X,\n",
        "        discriminator_Y,\n",
        "        lambda_cycle=10.0,\n",
        "        lambda_identity=0.5,\n",
        "    ):\n",
        "        super(CycleGan, self).__init__()\n",
        "        self.gen_G = generator_G\n",
        "        self.gen_F = generator_F\n",
        "        self.disc_X = discriminator_X\n",
        "        self.disc_Y = discriminator_Y\n",
        "        self.lambda_cycle = lambda_cycle\n",
        "        self.lambda_identity = lambda_identity\n",
        "\n",
        "    def compile(\n",
        "        self,\n",
        "        gen_G_optimizer,\n",
        "        gen_F_optimizer,\n",
        "        disc_X_optimizer,\n",
        "        disc_Y_optimizer,\n",
        "        gen_loss_fn,\n",
        "        disc_loss_fn,\n",
        "    ):\n",
        "        super(CycleGan, self).compile()\n",
        "        self.gen_G_optimizer = gen_G_optimizer\n",
        "        self.gen_F_optimizer = gen_F_optimizer\n",
        "        self.disc_X_optimizer = disc_X_optimizer\n",
        "        self.disc_Y_optimizer = disc_Y_optimizer\n",
        "        self.generator_loss_fn = gen_loss_fn\n",
        "        self.discriminator_loss_fn = disc_loss_fn\n",
        "        self.cycle_loss_fn = keras.losses.MeanAbsoluteError()\n",
        "        self.identity_loss_fn = keras.losses.MeanAbsoluteError()\n",
        "\n",
        "    def train_step(self, batch_data):\n",
        "        # x is Horse and y is zebra\n",
        "        real_x, real_y = batch_data\n",
        "\n",
        "        # For CycleGAN, we need to calculate different\n",
        "        # kinds of losses for the generators and discriminators.\n",
        "        # We will perform the following steps here:\n",
        "        #\n",
        "        # 1. Pass real images through the generators and get the generated images\n",
        "        # 2. Pass the generated images back to the generators to check if we\n",
        "        #    we can predict the original image from the generated image.\n",
        "        # 3. Do an identity mapping of the real images using the generators.\n",
        "        # 4. Pass the generated images in 1) to the corresponding discriminators.\n",
        "        # 5. Calculate the generators total loss (adverserial + cycle + identity)\n",
        "        # 6. Calculate the discriminators loss\n",
        "        # 7. Update the weights of the generators\n",
        "        # 8. Update the weights of the discriminators\n",
        "        # 9. Return the losses in a dictionary\n",
        "\n",
        "        with tf.GradientTape(persistent=True) as tape:\n",
        "            # Horse to fake zebra\n",
        "            fake_y = self.gen_G(real_x, training=True)\n",
        "            # Zebra to fake horse -> y2x\n",
        "            fake_x = self.gen_F(real_y, training=True)\n",
        "\n",
        "            # Cycle (Horse to fake zebra to fake horse): x -> y -> x\n",
        "            cycled_x = self.gen_F(fake_y, training=True)\n",
        "            # Cycle (Zebra to fake horse to fake zebra) y -> x -> y\n",
        "            cycled_y = self.gen_G(fake_x, training=True)\n",
        "\n",
        "            # Identity mapping\n",
        "            same_x = self.gen_F(real_x, training=True)\n",
        "            same_y = self.gen_G(real_y, training=True)\n",
        "\n",
        "            # Discriminator output\n",
        "            disc_real_x = self.disc_X(real_x, training=True)\n",
        "            disc_fake_x = self.disc_X(fake_x, training=True)\n",
        "\n",
        "            disc_real_y = self.disc_Y(real_y, training=True)\n",
        "            disc_fake_y = self.disc_Y(fake_y, training=True)\n",
        "\n",
        "            # Generator adverserial loss\n",
        "            gen_G_loss = self.generator_loss_fn(disc_fake_y)\n",
        "            gen_F_loss = self.generator_loss_fn(disc_fake_x)\n",
        "\n",
        "            # Generator cycle loss\n",
        "            cycle_loss_G = self.cycle_loss_fn(real_y, cycled_y) * self.lambda_cycle\n",
        "            cycle_loss_F = self.cycle_loss_fn(real_x, cycled_x) * self.lambda_cycle\n",
        "\n",
        "            # Generator identity loss\n",
        "            id_loss_G = (\n",
        "                self.identity_loss_fn(real_y, same_y)\n",
        "                * self.lambda_cycle\n",
        "                * self.lambda_identity\n",
        "            )\n",
        "            id_loss_F = (\n",
        "                self.identity_loss_fn(real_x, same_x)\n",
        "                * self.lambda_cycle\n",
        "                * self.lambda_identity\n",
        "            )\n",
        "\n",
        "            # Total generator loss\n",
        "            total_loss_G = gen_G_loss + cycle_loss_G + id_loss_G\n",
        "            total_loss_F = gen_F_loss + cycle_loss_F + id_loss_F\n",
        "\n",
        "            # Discriminator loss\n",
        "            disc_X_loss = self.discriminator_loss_fn(disc_real_x, disc_fake_x)\n",
        "            disc_Y_loss = self.discriminator_loss_fn(disc_real_y, disc_fake_y)\n",
        "\n",
        "        # Get the gradients for the generators\n",
        "        grads_G = tape.gradient(total_loss_G, self.gen_G.trainable_variables)\n",
        "        grads_F = tape.gradient(total_loss_F, self.gen_F.trainable_variables)\n",
        "\n",
        "        # Get the gradients for the discriminators\n",
        "        disc_X_grads = tape.gradient(disc_X_loss, self.disc_X.trainable_variables)\n",
        "        disc_Y_grads = tape.gradient(disc_Y_loss, self.disc_Y.trainable_variables)\n",
        "\n",
        "        # Update the weights of the generators\n",
        "        self.gen_G_optimizer.apply_gradients(\n",
        "            zip(grads_G, self.gen_G.trainable_variables)\n",
        "        )\n",
        "        self.gen_F_optimizer.apply_gradients(\n",
        "            zip(grads_F, self.gen_F.trainable_variables)\n",
        "        )\n",
        "\n",
        "        # Update the weights of the discriminators\n",
        "        self.disc_X_optimizer.apply_gradients(\n",
        "            zip(disc_X_grads, self.disc_X.trainable_variables)\n",
        "        )\n",
        "        self.disc_Y_optimizer.apply_gradients(\n",
        "            zip(disc_Y_grads, self.disc_Y.trainable_variables)\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"G_loss\": total_loss_G,\n",
        "            \"F_loss\": total_loss_F,\n",
        "            \"D_X_loss\": disc_X_loss,\n",
        "            \"D_Y_loss\": disc_Y_loss,\n",
        "        }"
      ],
      "metadata": {
        "id": "t-Sxd48T6If6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GANMonitor(keras.callbacks.Callback):\n",
        "    \"\"\"A callback to generate and save images after each epoch\"\"\"\n",
        "\n",
        "    def __init__(self, num_img=4):\n",
        "        self.num_img = num_img\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        _, ax = plt.subplots(4, 2, figsize=(12, 12))\n",
        "        for i, img in enumerate(test_horses.take(self.num_img)):\n",
        "            prediction = self.model.gen_G(img)[0].numpy()\n",
        "            prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n",
        "            img = (img[0] * 127.5 + 127.5).numpy().astype(np.uint8)\n",
        "\n",
        "            ax[i, 0].imshow(img)\n",
        "            ax[i, 1].imshow(prediction)\n",
        "            ax[i, 0].set_title(\"Input image\")\n",
        "            ax[i, 1].set_title(\"Translated image\")\n",
        "            ax[i, 0].axis(\"off\")\n",
        "            ax[i, 1].axis(\"off\")\n",
        "\n",
        "            prediction = keras.preprocessing.image.array_to_img(prediction)\n",
        "            prediction.save(\n",
        "                \"generated_img_{i}_{epoch}.png\".format(i=i, epoch=epoch + 1)\n",
        "            )\n",
        "        plt.show()\n",
        "        plt.close()\n"
      ],
      "metadata": {
        "id": "7rHLdBWh7D8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss functions "
      ],
      "metadata": {
        "id": "FuGSUVQl7zgT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss function for evaluating adversarial loss\n",
        "adv_loss_fn = keras.losses.MeanSquaredError()\n",
        "\n",
        "# Define the loss function for the generators\n",
        "def generator_loss_fn(fake):\n",
        "    fake_loss = adv_loss_fn(tf.ones_like(fake), fake)\n",
        "    return fake_loss\n",
        "\n",
        "\n",
        "# Define the loss function for the discriminators\n",
        "def discriminator_loss_fn(real, fake):\n",
        "    real_loss = adv_loss_fn(tf.ones_like(real), real)\n",
        "    fake_loss = adv_loss_fn(tf.zeros_like(fake), fake)\n",
        "    return (real_loss + fake_loss) * 0.5\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FjCo8l6U7GTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train Model"
      ],
      "metadata": {
        "id": "8sjKJSod727C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create cycle gan model\n",
        "cycle_gan_model = CycleGan(\n",
        "    generator_G=gen_G, generator_F=gen_F, discriminator_X=disc_X, discriminator_Y=disc_Y\n",
        ")\n",
        "\n",
        "# Compile the model\n",
        "cycle_gan_model.compile(\n",
        "    gen_G_optimizer=keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5),\n",
        "    gen_F_optimizer=keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5),\n",
        "    disc_X_optimizer=keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5),\n",
        "    disc_Y_optimizer=keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5),\n",
        "    gen_loss_fn=generator_loss_fn,\n",
        "    disc_loss_fn=discriminator_loss_fn,\n",
        ")\n",
        "# Callbacks\n",
        "plotter = GANMonitor()\n",
        "checkpoint_filepath = \"./model_checkpoints/cyclegan_checkpoints.{epoch:03d}\"\n",
        "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath\n",
        ")\n",
        "\n",
        "# Here we will train the model for just one epoch as each epoch takes around\n",
        "# 7 minutes on a single P100 backed machine.\n",
        "cycle_gan_model.fit(\n",
        "    tf.data.Dataset.zip((train_horses, train_zebras)),\n",
        "    epochs=1,\n",
        "    callbacks=[plotter, model_checkpoint_callback],\n",
        ")"
      ],
      "metadata": {
        "id": "H7TOby_S8G3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Cp7o1-3q8SCq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}