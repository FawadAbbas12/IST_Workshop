{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SESSION_03.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Session 03 "
      ],
      "metadata": {
        "id": "c1AlG8Br5-VP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Journey from Python toTensorflow <table class=\"tfo-notebook-buttons\" align=\"right\" style=\"margin-top:-55px\">\n",
        "<td>\n",
        "      <a target=\"_blank\" href=\"https://www.tensorflow.org/api_docs/python/tf/all_symbols\"><CNTER> <img src=\"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fupload.wikimedia.org%2Fwikipedia%2Fcommons%2Fthumb%2F2%2F2d%2FTensorflow_logo.svg%2F957px-Tensorflow_logo.svg.png&f=1&nofb=1\"  width=\"50\" height=\"50\" /><p style='margin-left:12px'>__TF__</p></CENTER></a>\n",
        "  </td>\n",
        "  <td>\n",
        "      <a target=\"_blank\" href=\"https://keras.io/guides/\"><CNTER><img src=\"https://external-content.duckduckgo.com/iu/?u=http%3A%2F%2Fadventuresinmachinelearning.com%2Fwp-content%2Fuploads%2F2017%2F05%2Fkeras-logo-small-wb-1.png&f=1&nofb=1\"  width=\"50\" height=\"50\" /><p style='margin-left:12px'>KERAS</p></CENTER></a>\n",
        "  </td>\n",
        " </table>"
      ],
      "metadata": {
        "id": "51CugV4mK7dS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/FawadAbbas12/IST_Workshop\n",
        "!mv IST_Workshop res"
      ],
      "metadata": {
        "id": "vaxus0lz4gnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "a7b75bdc"
      },
      "source": [
        "## TF Data representation ⛏\n",
        "tf.constant will create tensors wiht constant on the current device.\n",
        "\n",
        "    tf.constant(\n",
        "        value, dtype=None, shape=None, name='Const'\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-06T10:21:36.675918Z",
          "start_time": "2022-01-06T10:21:36.672203Z"
        },
        "hidden": true,
        "id": "68be2cc3"
      },
      "outputs": [],
      "source": [
        "# importing tensorflow\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "R_Channel_offset = tf.constant(3, dtype=tf.int32)\n",
        "print(R_Channel_offset)\n",
        "\n",
        "grid = np.array([[1,0,0],[0,1,0],[0,0,1]])\n",
        "grid_tensor = tf.constant(grid)\n",
        "print(grid_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "bbc53b8b"
      },
      "source": [
        "### Vaiables\n",
        "tf.Variable creates a ensor wiht viable value so it can be chaged during session execution \n",
        "\n",
        "    tf.Variable(\n",
        "    initial_value=None, trainable=None, validate_shape=True, caching_device=None,\n",
        "    name=None, variable_def=None, dtype=None, import_scope=None, constraint=None,\n",
        "    synchronization=tf.VariableSynchronization.AUTO,\n",
        "    aggregation=tf.compat.v1.VariableAggregation.NONE, shape=None\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-06T10:29:29.655226Z",
          "start_time": "2022-01-06T10:29:29.650775Z"
        },
        "hidden": true,
        "id": "6ce0f834"
      },
      "outputs": [],
      "source": [
        "layer1_bias = tf.Variable(initial_value=0, dtype=tf.float64)\n",
        "layer1_bias.assign(.001)\n",
        "print(layer1_bias)\n",
        "print(layer1_bias.value())\n",
        "\n",
        "non_trainable = tf.Variable(.01, trainable=False)\n",
        "print(non_trainable)\n",
        "print(non_trainable.value())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "4b81f34d"
      },
      "source": [
        "### Place holders\n",
        "place holders are used to feed values further into computation\n",
        "\n",
        "    tf.raw_ops.Placeholder(\n",
        "        dtype, shape=None, name=None\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "a33dc38a"
      },
      "source": [
        "## GradientTape\n",
        "Gradient tape is ued to automatically compute differentials for operations which were performaed under it's scope\n",
        "\n",
        "    tf.GradientTape(\n",
        "    persistent=False, watch_accessed_variables=True\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-06T10:45:59.340579Z",
          "start_time": "2022-01-06T10:45:59.336217Z"
        },
        "hidden": true,
        "id": "dcea4640"
      },
      "outputs": [],
      "source": [
        "offset = tf.constant(3.)\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "    trainable = tf.Variable(1.)\n",
        "    non_trainable = tf.Variable(2., trainable=False)\n",
        "    x1 = trainable * 2.\n",
        "    x1 = x1 + offset\n",
        "    x2 = non_trainable * 3.\n",
        "print(x1)    \n",
        "print(tape.gradient(x1, trainable))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classical Classification ⚒ "
      ],
      "metadata": {
        "id": "Ch9iGXGrP6vP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### LinearClassifier Example with perceptron training etc\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.datasets import make_circles\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier \n",
        "from mlxtend.plotting import plot_decision_regions \n",
        "from sklearn import metrics\n",
        "\n",
        "def plotResults(x, Y, model, title, plot_index):\n",
        "    plt.subplot(1,3,plot_index)\n",
        "    plot_decision_regions(x,Y,model,legend=2)\n",
        "    plt.title(title)"
      ],
      "metadata": {
        "id": "2a4sRpD4jXST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.model_selection import cross_val_score\n",
        "\n",
        "x,Y = make_classification(n_features=2,n_classes=2,n_samples=100,n_redundant=0,n_clusters_per_class=1)\n",
        "# x,Y = make_circles(n_samples=100,noise=0.03,factor=0.7)\n",
        "\n",
        "clf_RF = RandomForestClassifier()\n",
        "clf_LR = LogisticRegression()\n",
        "clf_SVC = SVC() \n",
        "\n",
        "clf_RF.fit(x,Y)\n",
        "clf_RF_acc = clf_RF.score(x,Y) \n",
        "\n",
        "clf_LR.fit(x,Y)\n",
        "clf_LR_acc = clf_LR.score(x,Y) \n",
        "# clf_LR_acc = max(cross_val_score(clf_LR,x,Y,cv=5,scoring='accuracy'))\n",
        "\n",
        "clf_SVC.fit(x,Y)\n",
        "clf_SVC_acc = clf_SVC.score(x,Y) \n",
        "# clf_SVC_acc = max(cross_val_score(clf_SVC,x,Y,cv=5,scoring='accuracy'))\n",
        "plt.figure(figsize=(8, 4.5))\n",
        "plotResults(x, Y, clf_RF, \"LR: Acc= \"+str(clf_LR_acc*100.), 1)\n",
        "plotResults(x, Y, clf_LR, \"RF: Acc= \"+str(clf_RF_acc*100.), 2)\n",
        "plotResults(x, Y, clf_SVC, \"SVM: Acc= \"+str(clf_SVC_acc*100.), 3)\n",
        "plt.show()\n",
        "\n",
        "#######################################\n",
        "# exercise 1 uncomment line# 4 and    #\n",
        "# rerun this code block               #\n",
        "#######################################\n",
        "\n",
        "#######################################\n",
        "# exercise 2 uncomment line# 15&19 and#\n",
        "# rerun this code block               #\n",
        "#######################################\n"
      ],
      "metadata": {
        "id": "ddO4S8vS55gt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm = metrics.confusion_matrix(Y,clf_RF.predict(x))\n",
        "import seaborn as sns\n",
        "sns.heatmap(cm,annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r')\n",
        "plt.ylabel('Actual label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oOT_zfURmMGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tensorflow and Keras  <img src='https://raw.githubusercontent.com/FawadAbbas12/Workshop_ses1/main/img.png' width=40px />"
      ],
      "metadata": {
        "id": "Iu20Gl3aWvng"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loss functions \n",
        "In the context of an optimization algorithm, the function used to evaluate a candidate solution (i.e. a set of weights) is referred to as the objective function. Typically, with neural networks, we seek to minimize the error. As such, the objective function is often referred to as a cost function or a loss function and the value calculated by the loss function is referred to as simply loss. The cost or loss function has an important job in that it must faithfully distill all aspects of the model down into a single number in such a way that improvements in that number are a sign of a better model.\n",
        " \n",
        "Keras have following prebuild loss functoin \n",
        "* Probabilistic losses\n",
        "        BinaryCrossentropy \n",
        "        CategoricalCrossentropy \n",
        "        SparseCategoricalCrossentropy \n",
        "        Poisson \n",
        "        KLDivergence \n",
        "\n",
        "* Regression losses\n",
        "        MeanSquaredError \n",
        "        MeanAbsoluteError \n",
        "        MeanAbsolutePercentageError \n",
        "        MeanSquaredLogarithmicError \n",
        "        CosineSimilarity \n",
        "        Huber \n"
      ],
      "metadata": {
        "id": "AUxSnhWw3VFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sequential API "
      ],
      "metadata": {
        "id": "K4zvaSK64Z6z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification,make_circles\n",
        "\n",
        "x,Y = make_classification(n_features=2,n_classes=2,n_samples=100,n_redundant=0,n_clusters_per_class=1)\n",
        "# x,Y = make_circles(n_samples=100,noise=0.03,factor=0.7)\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    # tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    loss=tf.keras.losses.binary_crossentropy,\n",
        "    optimizer=tf.keras.optimizers.Adam(lr=0.03),\n",
        "    metrics=[\n",
        "        tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "        tf.keras.metrics.Precision(name='precision'),\n",
        "        tf.keras.metrics.Recall(name='recall')\n",
        "    ]\n",
        ")\n",
        "\n",
        "history = model.fit(x, Y, epochs=100, verbose=0)\n",
        "accuracy = max(history.history.get('accuracy'))*100\n",
        "from mlxtend.plotting import plot_decision_regions\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plot_decision_regions(x, Y, clf=model, legend=2)\n",
        "plt.suptitle('Accuracy'+str(accuracy))\n",
        "plt.show()\n",
        "\n",
        "#######################################\n",
        "# exercise 1 uncomment line# 4 and    #\n",
        "# rerun this code block               #\n",
        "#######################################\n",
        "\n",
        "#######################################\n",
        "# exercise 2 uncomment line# 10 and   #\n",
        "# rerun this code block               #\n",
        "#######################################"
      ],
      "metadata": {
        "id": "GzrpVgENbhl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task"
      ],
      "metadata": {
        "id": "ilOkr-fADQIS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "###########################################\n",
        "# exercise 1 Using keras Sequential API   #\n",
        "# create a model with following layers    #\n",
        "# 1) Dense => neurons= 50,input_shape=50, #\n",
        "#    activation relu                      #\n",
        "# 2) Dense => neurons= 150                #\n",
        "#    activation relu                      #\n",
        "# 3) Dense => neurons 10,                 # \n",
        "#    activatoin softmax                   #\n",
        "###########################################\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(units=1, activation='tanh', input_shape=(10,)),\n",
        "]);\n",
        "\n",
        "plot_model(model, show_shapes=True)"
      ],
      "metadata": {
        "id": "Eubpju61DTUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Keras Callback functions \n",
        "A callback is an object that can perform actions at various stages of training\n",
        "(e.g. at the start or end of an epoch, before or after a single batch, etc).\n",
        "\n",
        "[More here](https://keras.io/api/callbacks/)\n",
        "\n",
        "You can use callbacks to:\n",
        "\n",
        "    Write TensorBoard logs after every batch of training to monitor your metrics\n",
        "    Periodically save your model to disk\n",
        "    Do early stopping\n",
        "    Get a view on internal states and statistics of a model during training\n",
        "    ...and more\n"
      ],
      "metadata": {
        "id": "9Qw1LQdBo7m-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# my_callbacks = [\n",
        "#     tf.keras.callbacks.EarlyStopping(patience=2),\n",
        "#     tf.keras.callbacks.ModelCheckpoint(filepath='model.{epoch:02d}-{val_loss:.2f}.h5'),\n",
        "#     tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
        "# ]\n",
        "# model.fit(dataset, epochs=10, callbacks=my_callbacks)"
      ],
      "metadata": {
        "id": "VzVIf5hgo7b1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save and load model 💾"
      ],
      "metadata": {
        "id": "WkxjuhFr3ZeR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(128, input_shape=(28, 28)),\n",
        "    tf.keras.layers.Activation(\"relu\"),\n",
        "    tf.keras.layers.Dense(1),\n",
        "    tf.keras.layers.Activation(\"softmax\")\n",
        "])\n",
        "model.summary()\n",
        "model.save('./res/savedModel.hd5')\n"
      ],
      "metadata": {
        "id": "mfMTvjrw3Y7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.utils import plot_model\n",
        "new_model = load_model('./res/savedModel.hd5')\n",
        "plot_model(new_model, show_shapes=True)"
      ],
      "metadata": {
        "id": "oeZtOiS-5hWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tensorboard\n",
        "TensorBoard provides the visualization and tooling needed for machine learning experimentation:\n",
        "\n",
        "*   Tracking and visualizing metrics such as loss and accuracy\n",
        "*Visualizing the model graph (ops and layers)\n",
        "*Viewing histograms of weights, biases, or other tensors as they change over time\n",
        "*Projecting embeddings to a lower dimensional space\n",
        "*Displaying images, text, and audio data\n",
        "*Profiling TensorFlow programs\n",
        "\n",
        "Keras callback function for tensorboard \n",
        "\n",
        "    tf.keras.callbacks.TensorBoard(\n",
        "        log_dir=\"logs\",\n",
        "        histogram_freq=0,\n",
        "        write_graph=True,\n",
        "        write_images=False,\n",
        "        write_steps_per_second=False,\n",
        "        update_freq=\"epoch\",\n",
        "        profile_batch=0,\n",
        "        embeddings_freq=0,\n",
        "        embeddings_metadata=None,\n",
        "        **kwargs\n",
        "    )"
      ],
      "metadata": {
        "id": "ZFlf9VAgoigA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard\n",
        "import tensorflow as tf\n",
        "import datetime, os\n",
        "!mkdir logs"
      ],
      "metadata": {
        "id": "FkiyRr2x1-CX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TENSORBOARD_EXAMPLE():\n",
        "    def __init__(self, log_dir):\n",
        "        self.log_dir = log_dir\n",
        "        \n",
        "    def build_model(self):\n",
        "        return tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "        tf.keras.layers.Dense(512, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "        tf.keras.layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    def train_model(self, model):\n",
        "        self.model = model\n",
        "        model.compile(optimizer='adam',\n",
        "                        loss='sparse_categorical_crossentropy',\n",
        "                        metrics=['accuracy'])\n",
        "\n",
        "        tensorboard_callback = tf.keras.callbacks.TensorBoard(self.log_dir, histogram_freq=1, write_images=True)\n",
        "\n",
        "        model.fit(x=x_train, \n",
        "                    y=y_train, \n",
        "                    epochs=5, \n",
        "                    validation_data=(x_test, y_test), \n",
        "                    callbacks=[tensorboard_callback])\n",
        "\n",
        "#load dataset\n",
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "(x_train, y_train),(x_test, y_test) = fashion_mnist.load_data()\n",
        "#normalize datset\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "\n",
        "example = TENSORBOARD_EXAMPLE(logdir)\n",
        "model = example.build_model()\n",
        "example.train_model(model)"
      ],
      "metadata": {
        "id": "vWxQ4E5Iq2iK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir logs"
      ],
      "metadata": {
        "id": "_CCRwiozMB-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tips and Advance concepts 👇\n",
        "\n"
      ],
      "metadata": {
        "id": "9z4IAPmbFwF0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36190f05"
      },
      "source": [
        "## Online Data Loaders\n",
        "### tfds\n",
        "<code>tensorflow_datasets</code> (tfds) defines a collection of datasets ready-to-use with TensorFlow.\n",
        "Each dataset is defined as a <code>tfds.core.DatasetBuilder</code>, which encapsulates the logic to download the dataset and construct an input pipeline, as well as contains the dataset documentation (version, splits, number of examples, etc.).\n",
        "\n",
        "    The main library entrypoints are:\n",
        "        tfds.builder: fetch a tfds.core.DatasetBuilder by name\n",
        "        tfds.load: convenience method to construct a builder, download the data, \n",
        "        and create an input pipeline, returning a tf.data.Dataset.\n",
        "\n",
        "### Instaltion\n",
        "    stable: pip install tensorflow-datasets\n",
        "    BETA  : pip install tfds-nightly\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-datasets >/dev/null"
      ],
      "metadata": {
        "id": "g4IfRypTvpm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "dc197c88"
      },
      "source": [
        "### Find datasets\n",
        "<code> tfds.list_builders() </code> returnes a list of all avaiable data builders under tfds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-07T04:40:04.264304Z",
          "start_time": "2022-01-07T04:40:04.259885Z"
        },
        "collapsed": true,
        "hidden": true,
        "id": "10c81530"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "avaiable_builders = tfds.list_builders()\n",
        "if 'cifar10' in avaiable_builders:\n",
        "    print(\"cifar10 is now supported by tfds\")\n",
        "print(f'total number of dataset builders: {len(avaiable_builders)}')\n",
        "print(f'list of all avaiable datasets is as following\\n {avaiable_builders}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b26db65"
      },
      "source": [
        "### Load online dataset\n",
        "<code> tfds.load </code> is one of the main entry point to load datasets it returnes <code>tf.data.Dataset</code>.\n",
        "\n",
        "    tfds.load(\n",
        "        name: str,\n",
        "        *,\n",
        "        split: Optional[Tree[splits_lib.Split]] = None,\n",
        "        data_dir: Optional[str] = None,\n",
        "        batch_size: tfds.typing.Dim = None,\n",
        "        shuffle_files: bool = False,\n",
        "        download: bool = True,\n",
        "        as_supervised: bool = False,\n",
        "        decoders: Optional[TreeDict[decode.Decoder]] = None,\n",
        "        read_config: Optional[tfds.ReadConfig] = None,\n",
        "        with_info: bool = False,\n",
        "        builder_kwargs: Optional[Dict[str, Any]] = None,\n",
        "        download_and_prepare_kwargs: Optional[Dict[str, Any]] = None,\n",
        "        as_dataset_kwargs: Optional[Dict[str, Any]] = None,\n",
        "        try_gcs: bool = False\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-07T04:48:46.924382Z",
          "start_time": "2022-01-07T04:48:46.875315Z"
        },
        "id": "e0f51c4a"
      },
      "outputs": [],
      "source": [
        "# data_set = tfds.load('mnist', split='train', shuffle_files=True)\n",
        "#download: 162.17 MiB, generated: 132.40 MiB, total: 294.58 MiB\n",
        "data_set = tfds.load('cifar10', split='train', shuffle_files=True)\n",
        "print(data_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-07T05:05:05.958725Z",
          "start_time": "2022-01-07T05:05:05.936964Z"
        },
        "collapsed": true,
        "id": "37d74020"
      },
      "outputs": [],
      "source": [
        "#get dataset entries \n",
        "batch_1 = data_set.batch(1).prefetch(1)\n",
        "print(batch_1)\n",
        "for item in batch_1:\n",
        "    print(item)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-07T05:06:24.588729Z",
          "start_time": "2022-01-07T05:06:20.096739Z"
        },
        "collapsed": true,
        "id": "447c7ea6"
      },
      "outputs": [],
      "source": [
        "# tfds.benchmark(data_set, batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-07T05:18:59.969330Z",
          "start_time": "2022-01-07T05:18:59.719841Z"
        },
        "collapsed": true,
        "id": "66bcb253"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "data_set, info = tfds.load('cifar10', split='train', shuffle_files=True, with_info=True)\n",
        "#does not supports batched input or video \n",
        "figure = tfds.show_examples(data_set, info)\n",
        "\n",
        "print(info.features[\"label\"].num_classes)\n",
        "print(info.features[\"label\"].names)\n",
        "print(info.features[\"label\"].int2str(7))  # Human readable version (8 -> 'cat')\n",
        "print(info.features[\"label\"].str2int('automobile'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Local data loader 🛻\n",
        "<code> keras.preprocessing.image.ImageDataGenerator</code>"
      ],
      "metadata": {
        "id": "Ib-jSVhDPaUa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-07T05:58:53.898541Z",
          "start_time": "2022-01-07T05:58:53.761097Z"
        },
        "id": "6a46f28f"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "datagen = ImageDataGenerator()\n",
        "train_generator = datagen.flow_from_directory('res/dataset_dir/train/', class_mode='binary', batch_size=1)\n",
        "test_generator = datagen.flow_from_directory('res/dataset_dir/test/', class_mode='binary', batch_size=1)\n",
        "val_generator = datagen.flow_from_directory('res/dataset_dir/validation/', class_mode='binary', batch_size=1)\n",
        "img, lable =train_generator.next()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-07T05:55:14.696720Z",
          "start_time": "2022-01-07T05:55:14.688037Z"
        },
        "id": "b3d236e8"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# dataFrame = pd.read_csv('res/dataset_df/test.csv')\n",
        "# dataFrame.head()\n",
        "# datagen = ImageDataGenerator()\n",
        "# train_gen = datagen.flow_from_dataframe(\n",
        "#     dataframe=dataFrame, \n",
        "#     directory='./res/dataset_df/', \n",
        "#     x_col='path',\n",
        "#     y_col='label',\n",
        "\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Customized Data Generator/loader 🚚"
      ],
      "metadata": {
        "id": "50QagmJkOCpD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "class Image_Generator(tf.keras.utils.Sequence) :\n",
        "  \n",
        "  def __init__(self, image_filenames, labels, batch_size, dim, name='training') :\n",
        "    self.image_filenames = image_filenames\n",
        "    self.rstate =0 \n",
        "    self.labels = labels\n",
        "    self.batch_size = batch_size\n",
        "    self.dim = dim\n",
        "    self.name = name\n",
        "    self.count =0\n",
        "  \n",
        "  def __len__(self) :\n",
        "    return (np.ceil(len(self.image_filenames) / float(self.batch_size))).astype(np.int)\n",
        "  \n",
        "  def get_batch_labels(self, idx):\n",
        "    batch_y = self.labels[idx * self.batch_size : (idx+1) * self.batch_size]\n",
        "    return batch_y\n",
        " \n",
        "  def get_batch_features(self, idx):\n",
        "    batch_x = self.image_filenames[idx * self.batch_size : (idx+1) * self.batch_size]\n",
        "    self.count += 1\n",
        "    imcount = 0\n",
        "    images = []\n",
        "    for img in batch_x:\n",
        "        try:\n",
        "            image = cv2.imread('/content/data/{}.jpg'.format(str(img).zfill(6)))\n",
        "            img_array = cv2.resize(image,(self.dim,self.dim),cv2.INTER_AREA).reshape(self.dim,self.dim,3)\n",
        "            images.append(img_array)\n",
        "            imcount +=1\n",
        "        except:\n",
        "            images.append(np.zeros((self.dim, self.dim, 3), dtype=np.float32S))\n",
        "            imcount +=1\n",
        "        if(self.count == self.__len__()):\n",
        "          self.image_filenames, self.labels = shuffle(self.image_filenames, self.labels, random_state=self.rstate)\n",
        "          self.rstate += 1 \n",
        "          self.count=0\n",
        "    return np.array(images)/255.0\n",
        "  \n",
        "  def __getitem__(self, idx) :\n",
        "    batch_x = self.get_batch_features(idx)\n",
        "    batch_y = self.get_batch_labels(idx)\n",
        "    return batch_x, batch_y"
      ],
      "metadata": {
        "id": "Y4c_XUEyOGvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Keras Functional API 🤯"
      ],
      "metadata": {
        "id": "LDd9oXG3PSKE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import tensorflow.keras\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, Activation\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from tensorflow.keras.layers import Input, Lambda\n",
        "from tensorflow.keras.models import Model, load_model \n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "class MULTIOUTPUT_MODEL():\n",
        "    def __init__(self, inputShape, branch1_out, branch2_out, finalAct=\"softmax\"):\n",
        "        self.inputShape = inputShape\n",
        "        self.branch1_out = branch1_out\n",
        "        self.branch2_out = branch2_out\n",
        "        self.finalAct = finalAct\n",
        "        self.chanDim = 1\n",
        "\n",
        "    def build_backbone(self, input):\n",
        "        # CONV => RELU => POOL\n",
        "        x = Conv2D(16, (3, 3), padding=\"same\")(input)\n",
        "        x = MaxPooling2D(pool_size=(3, 3))(x)\n",
        "        x = BatchNormalization(axis=self.chanDim)(x)\n",
        "        x = Activation(\"relu\")(x)\n",
        "        x = Dropout(0.25)(x)\n",
        "\n",
        "        # CONV => RELU => POOL\n",
        "        x = Conv2D(32, (3, 3), padding=\"same\")(x)\n",
        "        x = MaxPooling2D(pool_size=(3, 3))(x)\n",
        "        x = BatchNormalization(axis=self.chanDim)(x)\n",
        "        x = Activation(\"relu\")(x)\n",
        "        x = Dropout(0.25)(x)\n",
        "        x_flat = Flatten(name='Flat')(x)\n",
        "        return x_flat\n",
        "    \n",
        "    def build_branch_1(self, continuing_layer):\n",
        "        x1 = Dense(512 ,name='Dense1_1')(continuing_layer)\n",
        "        x1 = Activation(\"relu\")(x1)\n",
        "        x1 = BatchNormalization()(x1)\n",
        "        x1 = Dropout(0.5)(x1)\n",
        "        x1 = Dense(256 ,name='Dense1_2',kernel_regularizer=l2(0.001))(x1)\n",
        "        x1 = Activation(\"relu\")(x1)\n",
        "        x1 = BatchNormalization()(x1)\n",
        "        x1 = Dropout(0.5)(x1)\n",
        "        x1 = Dense(self.branch1_out ,name='Dense1_3')(x1)\n",
        "        x1 = Activation(self.finalAct, name=\"b1\")(x1)\n",
        "        return x1\n",
        "\n",
        "    def build_branch_2(self, continuing_layer):\n",
        "        x1 = Dense(512 ,name='Dense2_1')(continuing_layer)\n",
        "        x1 = Activation(\"relu\")(x1)\n",
        "        x1 = BatchNormalization()(x1)\n",
        "        x1 = Dropout(0.5)(x1)\n",
        "        x1 = Dense(256 ,name='Dense2_2',kernel_regularizer=l2(0.001))(x1)\n",
        "        x1 = Activation(\"relu\")(x1)\n",
        "        x1 = BatchNormalization()(x1)\n",
        "        x1 = Dropout(0.5)(x1)\n",
        "        x1 = Dense(self.branch2_out ,name='Dense2_3')(x1)\n",
        "        x1 = Activation(self.finalAct, name=\"b2\")(x1)\n",
        "        return x1\n",
        "\n",
        "    def build_model(self):\n",
        "        input = Input(shape=self.inputShape)\n",
        "        backBone = self.build_backbone(input)\n",
        "        brach1 = self.build_branch_1(backBone)\n",
        "        brach2 = self.build_branch_2(backBone)\n",
        "        model = Model(inputs=input, outputs=[brach1, brach2], name='MULTI_OUT')\n",
        "        return model \n",
        "\n",
        "\n",
        "plot_model(MULTIOUTPUT_MODEL((32, 32, 3), 54, 12).build_model(), show_shapes=True)"
      ],
      "metadata": {
        "id": "qE-5-Hc0PUo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Customized Keras callbacks 🚀"
      ],
      "metadata": {
        "id": "2-_fLZRMpDab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "class CustomCallback(keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print(\"Starting training; got log keys: {}\".format(keys))\n",
        "\n",
        "    def on_train_end(self, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print(\"Stop training; got log keys: {}\".format(keys))\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print(\"Start epoch {} of training; got log keys: {}\".format(epoch, keys))\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print(\"End epoch {} of training; got log keys: {}\".format(epoch, keys))\n",
        "\n",
        "    def on_test_begin(self, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print(\"Start testing; got log keys: {}\".format(keys))\n",
        "\n",
        "    def on_test_end(self, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print(\"Stop testing; got log keys: {}\".format(keys))\n",
        "\n",
        "    def on_predict_begin(self, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print(\"Start predicting; got log keys: {}\".format(keys))\n",
        "\n",
        "    def on_predict_end(self, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print(\"Stop predicting; got log keys: {}\".format(keys))\n",
        "\n",
        "    def on_train_batch_begin(self, batch, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print(\"...Training: start of batch {}; got log keys: {}\".format(batch, keys))\n",
        "\n",
        "    def on_train_batch_end(self, batch, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print(\"...Training: end of batch {}; got log keys: {}\".format(batch, keys))\n",
        "\n",
        "    def on_test_batch_begin(self, batch, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print(\"...Evaluating: start of batch {}; got log keys: {}\".format(batch, keys))\n",
        "\n",
        "    def on_test_batch_end(self, batch, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print(\"...Evaluating: end of batch {}; got log keys: {}\".format(batch, keys))\n",
        "\n",
        "    def on_predict_batch_begin(self, batch, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print(\"...Predicting: start of batch {}; got log keys: {}\".format(batch, keys))\n",
        "\n",
        "    def on_predict_batch_end(self, batch, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print(\"...Predicting: end of batch {}; got log keys: {}\".format(batch, keys))\n"
      ],
      "metadata": {
        "id": "GpZSKu95pIYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Dh1KSQPOpXFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa7f7c71"
      },
      "source": [
        "# Save and Close"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-06T08:01:26.053621Z",
          "start_time": "2022-01-06T08:01:26.032301Z"
        },
        "id": "7c1ba051"
      },
      "outputs": [],
      "source": [
        "%%javascript\n",
        "IPython.notebook.save_checkpoint();\n",
        "window.onbeforeunload = null\n",
        "window.close();\n",
        "IPython.notebook.session.delete();"
      ]
    }
  ]
}